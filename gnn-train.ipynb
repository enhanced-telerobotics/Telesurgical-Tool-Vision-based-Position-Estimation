{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.spatial.distance import cdist\n",
    "from GNN import StaticGNN as GNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# paths pre-setting\n",
    "label_root = f'/home/{os.getlogin()}/ERIE/realistic/output'\n",
    "model_root = f'/home/{os.getlogin()}/ERIE/realistic/Track-Shuyuan-2023-08-13/videos'\n",
    "model_name = 'DLC_resnet50_TrackAug13shuffle1_70000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# pre setting edges sets\n",
    "edges = [(0,1),(0,2),(1,3),(2,3),(3,4),(3,5),(4,6),(5,7),\n",
    "         (8,9),(8,10),(9,11),(10,11),(11,12),(11,13),(12,14),(13,15),\n",
    "         (0,8),(1,9),(2,10),(3,11),(4,12),(5,13),(6,14),(7,15)]\n",
    "onehot_matrix = np.eye(16)\n",
    "\n",
    "edge_index = np.zeros((2, len(edges)), dtype=np.int64)\n",
    "\n",
    "for i, edge in enumerate(edges):\n",
    "    edge_index[:, i] = edge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre setting training sets\n",
    "training_sets = [\"M1_L_V1_1\", \"M1_L_V1_2\", \"M1_R_V1_1\", \"M1_R_V1_2\",\n",
    "                 \"M3_L_V1_1\", \"M3_L_V1_2\", \"M3_R_V1_1\", \"M3_R_V1_2\", \n",
    "                 \"M5_L_V1_1\", \"M5_L_V1_2\", \"M5_R_V1_1\", \"M5_R_V1_2\"]\n",
    "\n",
    "# training_sets = [\"M1_R_V1_1\", \"M1_R_V1_2\",\n",
    "#                  \"M3_R_V1_1\", \"M3_R_V1_2\",\n",
    "#                  \"M5_R_V1_1\", \"M5_R_V1_2\"]\n",
    "\n",
    "# initial arrays\n",
    "X_train = np.zeros((0, 32))\n",
    "y_train = np.zeros((0, 3))\n",
    "\n",
    "for set in tqdm(training_sets):\n",
    "    # load from files\n",
    "    labels = np.genfromtxt(os.path.join(\n",
    "        label_root, set, 'labels_30hz.txt'), delimiter=',')\n",
    "    coordinates_L = pd.read_hdf(os.path.join(\n",
    "        model_root, f'{set}_L_h264{model_name}.h5'))\n",
    "    coordinates_R = pd.read_hdf(os.path.join(\n",
    "        model_root, f'{set}_R_h264{model_name}.h5'))\n",
    "\n",
    "    # unify size\n",
    "    frames = min(len(labels), len(coordinates_L), len(coordinates_R))\n",
    "\n",
    "    # drop and convert\n",
    "    coordinates_L = coordinates_L.filter(\n",
    "        regex='^(?!.*likelihood).*$', axis=1).to_numpy()[:frames]\n",
    "    coordinates_R = coordinates_R.filter(\n",
    "        regex='^(?!.*likelihood).*$', axis=1).to_numpy()[:frames]\n",
    "    labels = labels[:frames, 7:10]\n",
    "    coordinates = np.hstack((coordinates_L, coordinates_R))\n",
    "\n",
    "    X_train = np.vstack((X_train, MinMaxScaler().fit_transform(coordinates)))\n",
    "    y_train = np.vstack((y_train, MinMaxScaler().fit_transform(labels)))\n",
    "\n",
    "reshaped_X_train = X_train.reshape(-1, 16, 2)\n",
    "tiled_onehot = np.tile(onehot_matrix[:, :, np.newaxis], (1, 1, reshaped_X_train.shape[0])).transpose(2, 0, 1)\n",
    "X_train = np.concatenate((tiled_onehot, reshaped_X_train), axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, edge_index.shape, y_train.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GNN(input_dim=X_train.shape[2], hidden_dim=512, output_dim=y_train.shape[1],\n",
    "            device=device, num_hidden_layers=1, batch_size=512, l2_reg=0.0001, \n",
    "            lr=0.001, weights=[0.5, 1.0, 0.5], random_seed=random_seed,\n",
    "            message=\"smooth\")\n",
    "model.train(X_train, edge_index, y_train, \n",
    "            epochs=200, use_tqdm=True, save_loss=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "net_name = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(f'results/losses_{net_name}.json', 'r') as f:\n",
    "    params = json.load(f)\n",
    "\n",
    "input_dim = params[\"input_dim\"]\n",
    "hidden_dim = params[\"hidden_dim\"]\n",
    "output_dim = params[\"output_dim\"]\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "weights = params[\"weights\"]\n",
    "num_hidden_layers = params[\"num_hidden_layers\"]\n",
    "lr = params[\"lr\"]\n",
    "batch_size = params[\"batch_size\"]\n",
    "l2_reg = params[\"l2_reg\"]\n",
    "random_seed = params[\"random_seed\"]\n",
    "\n",
    "model = GNN(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, \n",
    "            device=device, num_hidden_layers=num_hidden_layers, batch_size=batch_size, \n",
    "            l2_reg=l2_reg, lr=lr, weights=weights, random_seed=random_seed,\n",
    "            message=\"realistic R+L\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\n",
    "    f'models/best_model_{net_name}.pth', map_location=device))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(X_train, edge_index, y_train, \n",
    "            epochs=200, use_tqdm=True, save_loss=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre setting test sets\n",
    "test_sets = [\"M1_L_V2_1\", \"M1_L_V2_2\", \"M1_R_V2_1\", \"M1_R_V2_2\",\n",
    "             \"M3_L_V2_1\", \"M3_L_V2_2\", \"M3_R_V2_1\", \"M3_R_V2_2\",\n",
    "             \"M5_L_V2_1\", \"M5_L_V2_2\", \"M5_R_V2_1\", \"M5_R_V2_2\",\n",
    "             \"M2_L_V1_1\", \"M2_L_V1_2\", \"M2_R_V1_1\", \"M2_R_V1_2\",\n",
    "             \"M4_L_V1_1\", \"M4_L_V1_2\", \"M4_R_V1_1\", \"M4_R_V1_2\",\n",
    "             \"M2_L_V2_1\", \"M2_L_V2_2\", \"M2_R_V2_1\", \"M2_R_V2_2\",\n",
    "             \"M4_L_V2_1\", \"M4_L_V2_2\", \"M4_R_V2_1\", \"M4_R_V2_2\"]\n",
    "\n",
    "# test_sets = [\"M1_R_V2_1\", \"M1_R_V2_2\",\n",
    "#              \"M3_R_V2_1\", \"M3_R_V2_2\",\n",
    "#              \"M5_R_V2_1\", \"M5_R_V2_2\",\n",
    "#              \"M2_R_V1_1\", \"M2_R_V1_2\",\n",
    "#              \"M4_R_V1_1\", \"M4_R_V1_2\",\n",
    "#              \"M2_R_V2_1\", \"M2_R_V2_2\",\n",
    "#              \"M4_R_V2_1\", \"M4_R_V2_2\"]\n",
    "\n",
    "rmse = np.zeros((3, len(test_sets)))\n",
    "\n",
    "for i, set in enumerate(test_sets):\n",
    "    # load from files\n",
    "    labels = np.genfromtxt(os.path.join(\n",
    "        label_root, set, 'labels_30hz.txt'), delimiter=',')\n",
    "    coordinates_L = pd.read_hdf(os.path.join(\n",
    "        model_root, f'{set}_L_h264{model_name}.h5'))\n",
    "    coordinates_R = pd.read_hdf(os.path.join(\n",
    "        model_root, f'{set}_R_h264{model_name}.h5'))\n",
    "\n",
    "    # unify size\n",
    "    frames = min(len(labels), len(coordinates_L), len(coordinates_R))\n",
    "\n",
    "    # drop and convert\n",
    "    coordinates_L = coordinates_L.filter(\n",
    "        regex='^(?!.*likelihood).*$', axis=1).to_numpy()[:frames]\n",
    "    coordinates_R = coordinates_R.filter(\n",
    "        regex='^(?!.*likelihood).*$', axis=1).to_numpy()[:frames]\n",
    "    labels = labels[:frames, 7:10]\n",
    "    coordinates = np.hstack((coordinates_L, coordinates_R))\n",
    "\n",
    "    X_test = MinMaxScaler().fit_transform(coordinates)\n",
    "    scaler = MinMaxScaler()\n",
    "    y_test = scaler.fit_transform(labels)\n",
    "\n",
    "    feature_matrices = []\n",
    "    for points in X_test.reshape(-1, 16, 2):\n",
    "        feature_matrices.append(np.hstack((onehot_matrix, points)))\n",
    "    X_test = np.array(feature_matrices)\n",
    "\n",
    "    y_pred = model.predict(X_test, edge_index, y_test)\n",
    "\n",
    "    y_test = scaler.inverse_transform(y_test)\n",
    "    y_pred = scaler.inverse_transform(y_pred)\n",
    "\n",
    "    # save prediction\n",
    "    # np.savetxt(f\"labels/{set}_gnn.txt\", y_pred, delimiter=\",\")\n",
    "\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    for axis in range(3):\n",
    "        rmse[axis, i] = mean_squared_error(\n",
    "            y_test[:, axis], y_pred[:, axis], squared=False)\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    fig, ax = plt.subplots(3, 1, figsize=(10, 8), sharex=True)\n",
    "    fig.suptitle(f\"Position - Time plot of imageset {set}\")\n",
    "    titles = ['X', 'Y', 'Z']\n",
    "    fps = 30  # Frames per second\n",
    "    time_values = [i/fps for i in range(labels.shape[0])]\n",
    "    for i in range(3):\n",
    "        ax[i].plot(time_values, y_test[:, i], color='black', label='actual')\n",
    "        ax[i].plot(time_values, y_pred[:, i], color='red', label='predict')\n",
    "        if i == 1:\n",
    "            ax[i].legend()\n",
    "        ax[i].set_ylabel(f'position {titles[i]}')\n",
    "        ax[i].set_xlim([0, np.max(time_values)])\n",
    "        # ax[i].set_ylim([-0.1, 1.1])\n",
    "\n",
    "    # Set the x-label only for the bottom subplot\n",
    "    ax[-1].set_xlabel('Time (s)')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    # plt.savefig(os.path.join(os.getcwd(), f'plots/{set}.pdf'), format='pdf')\n",
    "    import pickle\n",
    "    with open(os.path.join(os.getcwd(), f'plots/{set}_graph.pickle'), 'wb') as f:\n",
    "        pickle.dump(fig, f)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean and standard deviation of RMSE for all dimensions\n",
    "mean_rmse = np.mean(rmse, axis=1)\n",
    "std_rmse = np.std(rmse, axis=1)\n",
    "\n",
    "# Print mean and std for all dimensions\n",
    "print(\"Average RMSE (mean ± std):\")\n",
    "print(f\"{np.mean(mean_rmse):.3f} ± {np.mean(std_rmse):.3f}\")\n",
    "\n",
    "# Calculate mean and standard deviation of RMSE for each dimension and model\n",
    "dimensions = ['x', 'y', 'z']\n",
    "print(\"\\nRMSE (mean ± std) for each dimension:\")\n",
    "for i, dim in enumerate(dimensions):\n",
    "    print(f\"{dim}: {mean_rmse[i]:.3f} ± {std_rmse[i]:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
