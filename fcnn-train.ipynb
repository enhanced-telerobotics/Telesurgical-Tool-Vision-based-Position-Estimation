{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from FCNN import TwoInputFCNN as FCNN\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# paths pre-setting\n",
    "label_root = f'/home/{os.getlogin()}/ERIE/realistic/output'\n",
    "model_root = f'/home/{os.getlogin()}/ERIE/realistic/Track-Shuyuan-2023-08-13/videos'\n",
    "model_name = 'DLC_resnet50_TrackAug13shuffle1_70000'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre setting training sets\n",
    "training_sets = [\"M1_L_V1_1\", \"M1_L_V1_2\", \"M1_R_V1_1\", \"M1_R_V1_2\",\n",
    "                 \"M3_L_V1_1\", \"M3_L_V1_2\", \"M3_R_V1_1\", \"M3_R_V1_2\", \n",
    "                 \"M5_L_V1_1\", \"M5_L_V1_2\", \"M5_R_V1_1\", \"M5_R_V1_2\"]\n",
    "\n",
    "# training_sets = [\"M1_R_V1_1\", \"M1_R_V1_2\",\n",
    "#                  \"M3_R_V1_1\", \"M3_R_V1_2\", \n",
    "#                  \"M5_R_V1_1\", \"M5_R_V1_2\"]\n",
    "\n",
    "# initial arrays\n",
    "X_train_L = np.zeros((0, 16))\n",
    "X_train_R = np.zeros((0, 16))\n",
    "y_train = np.zeros((0, 3))\n",
    "\n",
    "for set in tqdm(training_sets):\n",
    "    # load from files\n",
    "    labels = np.genfromtxt(os.path.join(\n",
    "        label_root, set, 'labels_30hz.txt'), delimiter=',')\n",
    "    coordinates_L = pd.read_hdf(os.path.join(\n",
    "        model_root, f'{set}_L_h264{model_name}.h5'))\n",
    "    coordinates_R = pd.read_hdf(os.path.join(\n",
    "        model_root, f'{set}_R_h264{model_name}.h5'))\n",
    "\n",
    "    # unify size\n",
    "    frames = min(len(labels), len(coordinates_L), len(coordinates_R))\n",
    "\n",
    "    # drop and convert\n",
    "    coordinates_L = coordinates_L.filter(\n",
    "        regex='^(?!.*likelihood).*$', axis=1).to_numpy()[:frames]\n",
    "    coordinates_R = coordinates_R.filter(\n",
    "        regex='^(?!.*likelihood).*$', axis=1).to_numpy()[:frames]\n",
    "    labels = labels[:frames, 7:10]\n",
    "\n",
    "    X_train_L = np.vstack(\n",
    "        (X_train_L, MinMaxScaler().fit_transform(coordinates_L)))\n",
    "    X_train_R = np.vstack(\n",
    "        (X_train_R, MinMaxScaler().fit_transform(coordinates_R)))\n",
    "    y_train = np.vstack((y_train, MinMaxScaler().fit_transform(labels)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_L.shape, X_train_R.shape, y_train.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainning and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "net = FCNN(input_dim=X_train_L.shape[1], hidden_dim=16, output_dim=y_train.shape[1],\n",
    "           device=device, weights=[0.5, 1.0, 0.5], num_hidden_layers=4, batch_size=32,\n",
    "            l2_reg=0.0001, lr=0.0001, random_seed=42, message=\"smooth\")\n",
    "net.train(X_train_R, X_train_L, y_train,\n",
    "          epochs=200, use_tqdm=True, save_loss=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "net_name = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(f'results/losses_{net_name}.json', 'r') as f:\n",
    "    params = json.load(f)\n",
    "\n",
    "input_dim = params[\"input_dim\"]\n",
    "hidden_dim = params[\"hidden_dim\"]\n",
    "output_dim = params[\"output_dim\"]\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "weights = params[\"weights\"]\n",
    "num_hidden_layers = params[\"num_hidden_layers\"]\n",
    "lr = params[\"lr\"]\n",
    "l2_reg = params[\"l2_reg\"]\n",
    "batch_size = params[\"batch_size\"]\n",
    "random_seed = params[\"random_seed\"]\n",
    "\n",
    "net = FCNN(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim,\n",
    "           device=device, weights=weights, num_hidden_layers=num_hidden_layers,\n",
    "           batch_size=batch_size, lr=lr, l2_reg=l2_reg, random_seed=random_seed,\n",
    "           message=\"realistic R+L\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "net.load_state_dict(torch.load(\n",
    "    f'models/best_model_{net_name}.pth', map_location=device))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.train(X_train_R, X_train_L, y_train,\n",
    "          epochs=200, use_tqdm=True, save_loss=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre setting test sets\n",
    "test_sets = [\"M1_L_V2_1\", \"M1_L_V2_2\", \"M1_R_V2_1\", \"M1_R_V2_2\",\n",
    "             \"M3_L_V2_1\", \"M3_L_V2_2\", \"M3_R_V2_1\", \"M3_R_V2_2\",\n",
    "             \"M5_L_V2_1\", \"M5_L_V2_2\", \"M5_R_V2_1\", \"M5_R_V2_2\",\n",
    "             \"M2_L_V1_1\", \"M2_L_V1_2\", \"M2_R_V1_1\", \"M2_R_V1_2\",\n",
    "             \"M4_L_V1_1\", \"M4_L_V1_2\", \"M4_R_V1_1\", \"M4_R_V1_2\",\n",
    "             \"M2_L_V2_1\", \"M2_L_V2_2\", \"M2_R_V2_1\", \"M2_R_V2_2\",\n",
    "             \"M4_L_V2_1\", \"M4_L_V2_2\", \"M4_R_V2_1\", \"M4_R_V2_2\"]\n",
    "\n",
    "# test_sets = [\"M1_R_V2_1\", \"M1_R_V2_2\",\n",
    "#              \"M3_R_V2_1\", \"M3_R_V2_2\",\n",
    "#              \"M5_R_V2_1\", \"M5_R_V2_2\",\n",
    "#              \"M2_R_V1_1\", \"M2_R_V1_2\",\n",
    "#              \"M4_R_V1_1\", \"M4_R_V1_2\",\n",
    "#              \"M2_R_V2_1\", \"M2_R_V2_2\",\n",
    "#              \"M4_R_V2_1\", \"M4_R_V2_2\"]\n",
    "\n",
    "# initial arrays\n",
    "rmse = np.zeros((3, len(test_sets)))\n",
    "\n",
    "for i, set in enumerate(test_sets):\n",
    "    # load from files\n",
    "    labels = np.genfromtxt(os.path.join(\n",
    "        label_root, set, 'labels_30hz.txt'), delimiter=',')\n",
    "    coordinates_L = pd.read_hdf(os.path.join(\n",
    "        model_root, f'{set}_L_h264{model_name}.h5'))\n",
    "    coordinates_R = pd.read_hdf(os.path.join(\n",
    "        model_root, f'{set}_R_h264{model_name}.h5'))\n",
    "\n",
    "    # unify size\n",
    "    frames = min(len(labels), len(coordinates_L), len(coordinates_R))\n",
    "\n",
    "    # drop and convert\n",
    "    coordinates_L = coordinates_L.filter(\n",
    "        regex='^(?!.*likelihood).*$', axis=1).to_numpy()[:frames]\n",
    "    coordinates_R = coordinates_R.filter(\n",
    "        regex='^(?!.*likelihood).*$', axis=1).to_numpy()[:frames]\n",
    "    labels = labels[:frames, 7:10]\n",
    "\n",
    "    X_test_L = MinMaxScaler().fit_transform(coordinates_L)\n",
    "    X_test_R = MinMaxScaler().fit_transform(coordinates_R)\n",
    "    y_test = MinMaxScaler().fit_transform(labels)\n",
    "    y_pred = net.predict(X_test_R, X_test_L)\n",
    "\n",
    "    # save prediction\n",
    "    # np.savetxt(f\"labels/{set}_fcnn.txt\", y_pred, delimiter=\",\")\n",
    "\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    for axis in range(3):\n",
    "        rmse[axis, i] = mean_squared_error(y_test[:, axis], y_pred[:, axis], squared=False)\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    fig, ax = plt.subplots(3, 1, figsize=(10, 8), sharex=True)\n",
    "    fig.suptitle(f\"Position - Time plot of imageset {set}\")\n",
    "    titles = ['X', 'Y', 'X']\n",
    "    fps = 30  # Frames per second\n",
    "    time_values = [i/fps for i in range(y_test.shape[0])]\n",
    "    for i in range(3):\n",
    "        ax[i].plot(time_values, y_test[:, i], color='black', label='actual')\n",
    "        ax[i].plot(time_values, y_pred[:, i], color='red', label='predict')\n",
    "        if i == 1:\n",
    "            ax[i].legend()\n",
    "        ax[i].set_ylabel(f'norm position {titles[i]}')\n",
    "        ax[i].set_xlim([0, np.max(time_values)])\n",
    "        ax[i].set_ylim([-0.1, 1.1])\n",
    "\n",
    "    # Set the x-label only for the bottom subplot\n",
    "    ax[-1].set_xlabel('Time (s)')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    # plt.savefig(os.path.join(os.getcwd(), f'plots/{set}.pdf'), format='pdf')\n",
    "    import pickle\n",
    "    with open(os.path.join(os.getcwd(), f'plots/{set}_fcnn.pickle'), 'wb') as f:\n",
    "        pickle.dump(fig, f)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean and standard deviation of RMSE for all dimensions\n",
    "mean_rmse = np.mean(rmse, axis=1)\n",
    "std_rmse = np.std(rmse, axis=1)\n",
    "\n",
    "# Print mean and std for all dimensions\n",
    "print(\"Average RMSE (mean ± std):\")\n",
    "print(f\"{np.mean(mean_rmse):.3f} ± {np.mean(std_rmse):.3f}\")\n",
    "\n",
    "# Calculate mean and standard deviation of RMSE for each dimension and model\n",
    "dimensions = ['x', 'y', 'z']\n",
    "print(\"\\nRMSE (mean ± std) for each dimension:\")\n",
    "for i, dim in enumerate(dimensions):\n",
    "    print(f\"{dim}: {mean_rmse[i]:.3f} ± {std_rmse[i]:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
